# <p align="center">Optimization</p>

## ğŸ¯ Description

This project implements various optimization techniques for neural networks, including data normalization, shuffling, mini-batch processing, moving averages, momentum, RMSProp, Adam optimizer, learning rate decay, and batch normalization.

## âš™ï¸ Technologies

python3 (version 3.9)
numpy
tensorflow
keras

## ğŸ“ Clone the repository

```
git clone https://github.com/TonyHolby/holbertonschool-machine_learning.git
cd supervised_learning/optimization/
```

## ğŸ“„ Structure

```
ğŸ“‚ optimization/
â”œâ”€â”€ ğŸ“„ 0-norm_constants.py           # Normalization constants
â”œâ”€â”€ ğŸ“„ 1-normalize.py                # Data normalization
â”œâ”€â”€ ğŸ“„ 2-shuffle_data.py             # Data shuffling
â”œâ”€â”€ ğŸ“„ 3-mini_batch.py               # Mini-batch creation
â”œâ”€â”€ ğŸ“„ 4-moving_average.py           # Moving average calculation
â”œâ”€â”€ ğŸ“„ 5-momentum.py                 # Momentum optimizer
â”œâ”€â”€ ğŸ“„ 6-momentum.py                 # Advanced momentum
â”œâ”€â”€ ğŸ“„ 7-RMSProp.py                  # RMSProp optimizer
â”œâ”€â”€ ğŸ“„ 8-RMSProp.py                  # Advanced RMSProp
â”œâ”€â”€ ğŸ“„ 9-Adam.py                     # Adam optimizer
â”œâ”€â”€ ğŸ“„ 10-Adam.py                    # Advanced Adam
â”œâ”€â”€ ğŸ“„ 11-learning_rate_decay.py    # Learning rate decay
â”œâ”€â”€ ğŸ“„ 12-learning_rate_decay.py    # Advanced learning rate decay
â”œâ”€â”€ ğŸ“„ 13-batch_norm.py              # Batch normalization
â”œâ”€â”€ ğŸ“„ 14-batch_norm.py              # Advanced batch normalization
â”œâ”€â”€ ğŸ“„ MNIST.npz                     # MNIST dataset
â”œâ”€â”€ ğŸ“„ Binary_Train.npz              # Binary classification dataset
â”œâ”€â”€ ğŸ“„ model.h5                      # Trained model
â””â”€â”€ ğŸ“„ README.md                      # Project documentation
```

## ğŸ‘¤ Author

Tony NEMOUTHE
