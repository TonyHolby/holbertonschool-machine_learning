# <p align="center">Attention</p>

## ğŸ¯ Description

This project implements attention mechanisms and transformer architectures, including RNN encoders/decoders, self-attention, positional encoding, scaled dot-product attention, multi-head attention and complete transformer encoder/decoder blocks.

## âš™ï¸ Technologies

python3 (version 3.9)  
numpy (version 1.25.2)  
tensorflow (version 2.15.0)

## ğŸ“ Clone the repository

```
git clone https://github.com/TonyHolby/holbertonschool-machine_learning.git
cd supervised_learning/attention/
```

## ğŸ“„ Structure

```
ğŸ“‚ attention/
â”œâ”€â”€ ğŸ“„ 0-rnn_encoder.py                      # RNN encoder
â”œâ”€â”€ ğŸ“„ 1-self_attention.py                   # Self-attention mechanism
â”œâ”€â”€ ğŸ“„ 2-rnn_decoder.py                      # RNN decoder
â”œâ”€â”€ ğŸ“„ 4-positional_encoding.py              # Positional encoding for transformers
â”œâ”€â”€ ğŸ“„ 5-sdp_attention.py                    # Scaled dot-product attention
â”œâ”€â”€ ğŸ“„ 6-multihead_attention.py              # Multi-head attention mechanism
â”œâ”€â”€ ğŸ“„ 7-transformer_encoder_block.py        # Transformer encoder block
â”œâ”€â”€ ğŸ“„ 8-transformer_decoder_block.py        # Transformer decoder block
â”œâ”€â”€ ğŸ“„ 9-transformer_encoder.py              # Complete transformer encoder
â”œâ”€â”€ ğŸ“„ 10-transformer_decoder.py             # Complete transformer decoder
â”œâ”€â”€ ğŸ“„ 11-transformer.py                     # Full transformer model
â””â”€â”€ ğŸ“„ README.md                             # Project documentation
```

## ğŸ‘¤ Author

Tony NEMOUTHE
